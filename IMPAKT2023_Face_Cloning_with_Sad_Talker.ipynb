{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "# Introduction to SadTalker on Google Colab\n",
        "\n",
        "Welcome to the SadTalker interactive notebook on Google Colab! SadTalker is an advanced AI-driven tool that generates facial animations from still images synced with given audio. This technology uses state-of-the-art algorithms to analyze the content of the audio and then animates an image to match the speech or singing in the audio clip. Here’s what you need to know to get started:\n",
        "\n",
        "## How SadTalker Works\n",
        "\n",
        "Creating a realistic talking head video is tricky. Previous attempts often resulted in stiff facial expressions, awkward movements, or the character looking slightly different from the original photo.\n",
        "\n",
        "SadTalker tackles these challenges by learning how faces move in three dimensions (3D) when they talk. Instead of just copying the 2D movements seen in videos, it understands how the head tilts, turns, and how each part of the face moves while speaking.\n",
        "\n",
        "    3D Motion Coefficients: It carefully listens to the audio and figures out the 3D motion (like how the head moves and facial expressions) that would match the speech.\n",
        "\n",
        "    ExpNet: This part of SadTalker focuses on getting the expressions just right by learning from both the sound and how faces move in 3D.\n",
        "\n",
        "    PoseVAE: This component is in charge of creating realistic head movements to match the speaker’s style and emotion in the audio.\n",
        "\n",
        "    3D-Aware Face Render: After figuring out all the movements, SadTalker uses a special process to turn these 3D instructions into a video that looks smooth and natural.\n",
        "\n",
        "Read more about SadTalker here:\n",
        "[arxiv](https://arxiv.org/abs/2211.12194) | [project](https://sadtalker.github.io) | [Github](https://github.com/Winfredy/SadTalker)\n",
        "\n",
        "\n",
        "## What to Expect\n",
        "In this notebook, you will:\n",
        "\n",
        "1. Select an image: Use an example image provided by SadTalker or upload your own.\n",
        "2. Choose your driven audio: Start by using example audio clips provided or upload your own audio file (recommended to start with 1 minute of audio to ensure optimal processing time and resource use).\n",
        "3. Generate your SadTalker animation: Run the animation process to bring your image to life with your chosen audio.\n",
        "4. Download your video: Once the animation is complete, you can download the resulting video to your device.\n",
        "\n",
        "## Using the Notebook\n",
        "Follow the step-by-step instructions provided within the notebook. Initially, you'll select an image and an audio file. The notebook interface is interactive, allowing you to choose from provided examples or upload your own. When you're ready, you'll execute the code cells to run the SadTalker animation process.\n",
        "\n",
        "## Important Notes\n",
        "- **GPU Time Limitation**: Google Colab offers free access to GPUs but with usage limits. Be aware that the GPU runtime can disconnect if it exceeds these limits or if it detects inactivity for a certain period. Please save your work frequently.\n",
        "  \n",
        "- **Consequences of Inactivity**: If you’re inactive in the Colab notebook (i.e., not running any cells or interacting with the notebook), you may be disconnected from the GPU runtime. To avoid losing progress, ensure you're actively using the notebook or save your work regularly.\n",
        "\n",
        "- **Mindful Usage**: Given the limited GPU resources, please be considerate and use the resources judiciously. Lengthy or excessive use may prevent others from accessing these shared resources.\n",
        "\n",
        "## Ethical Considerations\n",
        "\n",
        "As you explore the capabilities of SadTalker, it's important to keep ethical considerations in mind:\n",
        "\n",
        "    Consent: Always have permission to use someone’s likeness and voice when creating talking head videos. Never use someone's image or voice without their explicit consent.\n",
        "\n",
        "    Avoidance of Deception: Be particularly careful to avoid using voice conversion to create deepfake audio or any form of media meant to deceive, manipulate, or mislead audiences.\n",
        "\n",
        "    Respect and Dignity: Ensure that the content created with SadTalker respects the dignity of the individuals whose images and voices are being used. Avoid creating content that could be considered defamatory, discriminatory, or offensive.\n",
        "\n",
        "    Deepfakes and Manipulation: Be aware of the implications of creating deepfakes — videos that could be used to deceive viewers. Always clearly label the content created with SadTalker as synthetic media.\n",
        "\n",
        "    Legal Compliance: Abide by all relevant laws and regulations regarding synthetic media, intellectual property rights, and personal privacy.\n",
        "\n",
        "    Responsible Sharing: When sharing content created with SadTalker, consider the impact it may have on the subjects depicted and society at large. Share responsibly and with a clear indication of the video's synthetic nature.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA89DV-sKS4i"
      },
      "source": [
        "Installation (around 5 mins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVYcelnCcEAn",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install Dependencies\n",
        "# Required Libraries\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "import subprocess\n",
        "import os\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML, display, clear_output\n",
        "import io\n",
        "from base64 import b64encode\n",
        "import os\n",
        "\n",
        "\n",
        "def start_time():\n",
        "  start = time.time()\n",
        "  current_time = datetime.now()\n",
        "  print(\"Current time:\", current_time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "  return start\n",
        "\n",
        "def elapsed_time(start):\n",
        "  print(\"Time (mins) it took to run this cell: \", round((time.time()- start)/60,2))\n",
        "\n",
        "\n",
        "def setup_environment():\n",
        "    # Update alternatives for Python\n",
        "    subprocess.run(['sudo', 'update-alternatives', '--install', '/usr/local/bin/python3', 'python3', '/usr/bin/python3.8', '2'])\n",
        "    subprocess.run(['sudo', 'update-alternatives', '--install', '/usr/local/bin/python3', 'python3', '/usr/bin/python3.9', '1'])\n",
        "    subprocess.run(['sudo', 'apt', 'install', 'python3.8'])\n",
        "    subprocess.run(['sudo', 'apt-get', 'install', 'python3.8-distutils'])\n",
        "    python_version = subprocess.check_output(['python', '--version'], universal_newlines=True)\n",
        "    print(python_version)\n",
        "    subprocess.run(['apt-get', 'update'])\n",
        "    subprocess.run(['apt', 'install', 'software-properties-common'])\n",
        "    subprocess.run(['sudo', 'dpkg', '--remove', '--force-remove-reinstreq', 'python3-pip', 'python3-setuptools', 'python3-wheel'])\n",
        "    subprocess.run(['apt-get', 'install', 'python3-pip'])\n",
        "\n",
        "    print('Git clone project and install requirements...')\n",
        "    subprocess.run(['git', 'clone', 'https://github.com/artificialnouveau/SadTalker'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    os.chdir('SadTalker')\n",
        "    os.environ['PYTHONPATH'] = '/content/SadTalker:' + os.environ.get('PYTHONPATH', '')\n",
        "    subprocess.run(['python3.8', '-m', 'pip', 'install', 'torch==1.12.1+cu113', 'torchvision==0.13.1+cu113', 'torchaudio==0.12.1', '--extra-index-url', 'https://download.pytorch.org/whl/cu113'])\n",
        "    subprocess.run(['apt', 'update'])\n",
        "    subprocess.run(['apt', 'install', 'ffmpeg'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    subprocess.run(['python3.8', '-m', 'pip', 'install', '-r', 'requirements.txt'])\n",
        "    print('Setup complete.')\n",
        "\n",
        "def create_image_audio_selector(img_list, audio_list):\n",
        "    selected_img = None\n",
        "    selected_audio = None\n",
        "\n",
        "    # Define a function to handle image and audio selection\n",
        "    def select_image_audio(change):\n",
        "        nonlocal selected_img, selected_audio\n",
        "\n",
        "        # Clear the output area\n",
        "        output.clear_output(wait=True)\n",
        "\n",
        "        if image_source.value == 'Select from Examples':\n",
        "            selected_img = 'examples/source_image/{}.png'.format(example_images.value)\n",
        "        elif image_source.value == 'Upload Image':\n",
        "            if uploaded_image.data:\n",
        "                selected_img = 'user_uploaded.png'\n",
        "                with open(selected_img, 'wb') as f:\n",
        "                    f.write(uploaded_image.data[-1])\n",
        "            else:\n",
        "                selected_img = 'examples/source_image/{}.png'.format(example_images.value)\n",
        "        else:\n",
        "            selected_img = '/content/gdrive/MyDrive/ColabNotebooks/' + colab_folder.value + '/{}.png'.format(example_images.value)\n",
        "\n",
        "        if audio_source.value == 'Select from Examples':\n",
        "            selected_audio = 'examples/driven_audio/{}.wav'.format(example_audio.value)\n",
        "        elif audio_source.value == 'Upload Audio':\n",
        "            if uploaded_audio.data:\n",
        "                selected_audio = 'user_uploaded.wav'\n",
        "                with open(selected_audio, 'wb') as f:\n",
        "                    f.write(uploaded_audio.data[-1])\n",
        "            else:\n",
        "                selected_audio = 'examples/driven_audio/{}.wav'.format(example_audio.value)\n",
        "        else:\n",
        "            selected_audio = '/content/gdrive/MyDrive/ColabNotebooks/' + colab_folder_audio.value + '/{}.wav'.format(example_audio.value)\n",
        "\n",
        "        img = plt.imread(selected_img)\n",
        "\n",
        "        with output:\n",
        "            # Display the selected image\n",
        "            plt.imshow(img)\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "            # Display the selected audio information\n",
        "            print(f'Selected Image: {selected_img}')\n",
        "            print(f'Selected Audio: {selected_audio}')\n",
        "\n",
        "    # Create an output widget for displaying the image and audio information\n",
        "    output = widgets.Output()\n",
        "\n",
        "    # Create a dropdown for image source selection\n",
        "    image_source = widgets.Dropdown(\n",
        "        options=['Select from Examples', 'Upload Image', 'Google Colab Folder'],\n",
        "        value='Select from Examples',\n",
        "        description='Image Source:',\n",
        "        disabled=False,\n",
        "    )\n",
        "\n",
        "    # Create a dropdown for audio source selection\n",
        "    audio_source = widgets.Dropdown(\n",
        "        options=['Select from Examples', 'Upload Audio', 'Google Colab Folder'],\n",
        "        value='Select from Examples',\n",
        "        description='Audio Source:',\n",
        "        disabled=False,\n",
        "    )\n",
        "\n",
        "    # Create a dropdown for selecting example images\n",
        "    example_images = widgets.Dropdown(\n",
        "        options=img_list,\n",
        "        value='full3',\n",
        "        description='Select Image:',\n",
        "        disabled=False,\n",
        "    )\n",
        "\n",
        "    # Create a dropdown for selecting example audio\n",
        "    example_audio = widgets.Dropdown(\n",
        "        options=audio_list,\n",
        "        value='RD_Radio31_000',\n",
        "        description='Select Audio:',\n",
        "        disabled=False,\n",
        "    )\n",
        "\n",
        "    # Create a file upload widget for images\n",
        "    uploaded_image = widgets.FileUpload(\n",
        "        accept='.png',\n",
        "        multiple=False,\n",
        "        description='Upload PNG Image:',\n",
        "        disabled=False\n",
        "    )\n",
        "\n",
        "    # Create a file upload widget for audio\n",
        "    uploaded_audio = widgets.FileUpload(\n",
        "        accept='.wav',\n",
        "        multiple=False,\n",
        "        description='Upload WAV Audio:',\n",
        "        disabled=False\n",
        "    )\n",
        "\n",
        "    # Create a text input widget for Colab folder path (image)\n",
        "    colab_folder = widgets.Text(\n",
        "        value='',\n",
        "        placeholder='Enter Colab Folder Path (Image)',\n",
        "        description='Colab Folder (Image):',\n",
        "        disabled=False\n",
        "    )\n",
        "\n",
        "    # Create a text input widget for Colab folder path (audio)\n",
        "    colab_folder_audio = widgets.Text(\n",
        "        value='',\n",
        "        placeholder='Enter Colab Folder Path (Audio)',\n",
        "        description='Colab Folder (Audio):',\n",
        "        disabled=False\n",
        "    )\n",
        "\n",
        "    # Add event handlers\n",
        "    image_source.observe(select_image_audio, names='value')\n",
        "    example_images.observe(select_image_audio, names='value')\n",
        "    uploaded_image.observe(select_image_audio, names='data')\n",
        "    audio_source.observe(select_image_audio, names='value')\n",
        "    example_audio.observe(select_image_audio, names='value')\n",
        "    uploaded_audio.observe(select_image_audio, names='data')\n",
        "\n",
        "    # Display widgets\n",
        "    display(image_source)\n",
        "    display(example_images)\n",
        "    display(uploaded_image)\n",
        "    display(colab_folder)\n",
        "    display(audio_source)\n",
        "    display(example_audio)\n",
        "    display(uploaded_audio)\n",
        "    display(colab_folder_audio)\n",
        "    display(output)\n",
        "\n",
        "    # Initial call to select_image_audio to display the default values\n",
        "    select_image_audio(None)\n",
        "\n",
        "    return selected_img, selected_audio\n",
        "\n",
        "\n",
        "# Function to display video with a download link\n",
        "def display_video_with_download_link(video_path):\n",
        "    # Read the video file\n",
        "    video_data = open(video_path, 'rb').read()\n",
        "\n",
        "    # Encode the video data in base64\n",
        "    video_base64 = b64encode(video_data).decode()\n",
        "\n",
        "    # Create a download link\n",
        "    download_link = f'<a href=\"data:video/mp4;base64,{video_base64}\" download=\"output_video.mp4\">Download Video</a>'\n",
        "\n",
        "    # Display the video and download link\n",
        "    video_html = f'''\n",
        "    <video width=\"640\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{video_base64}\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    <br>{download_link}\n",
        "    '''\n",
        "    display(HTML(video_html))\n",
        "\n",
        "\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_image_audio_selector(img_list, audio_list):\n",
        "    selected_img = None\n",
        "    selected_audio = None\n",
        "\n",
        "    # Define a function to handle image and audio selection\n",
        "    def select_image_audio(change):\n",
        "        nonlocal selected_img, selected_audio\n",
        "\n",
        "        # Clear the output area\n",
        "        output.clear_output(wait=True)\n",
        "\n",
        "        if image_source.value == 'Select from Examples':\n",
        "            selected_img = 'examples/source_image/{}.png'.format(example_images.value)\n",
        "        else:\n",
        "            selected_img = None\n",
        "\n",
        "        if audio_source.value == 'Select from Examples':\n",
        "            selected_audio = 'examples/driven_audio/{}.wav'.format(example_audio.value)\n",
        "        else:\n",
        "            selected_audio = None\n",
        "\n",
        "        if selected_img:\n",
        "            img = plt.imread(selected_img)\n",
        "            with output:\n",
        "                # Display the selected image\n",
        "                plt.imshow(img)\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "\n",
        "            # Display the selected audio information\n",
        "            if selected_audio:\n",
        "                print(f'Selected Image: {selected_img}')\n",
        "                print(f'Selected Audio: {selected_audio}')\n",
        "\n",
        "    # Create an output widget for displaying the image and audio information\n",
        "    output = widgets.Output()\n",
        "\n",
        "    # Create a dropdown for image source selection\n",
        "    image_source = widgets.Dropdown(\n",
        "        options=['Select from Examples'],\n",
        "        value='Select from Examples',\n",
        "        description='Image Source:',\n",
        "        disabled=False,\n",
        "    )\n",
        "\n",
        "    # Create a dropdown for audio source selection\n",
        "    audio_source = widgets.Dropdown(\n",
        "        options=['Select from Examples'],\n",
        "        value='Select from Examples',\n",
        "        description='Audio Source:',\n",
        "        disabled=False,\n",
        "    )\n",
        "\n",
        "    # Create a dropdown for selecting example images\n",
        "    example_images = widgets.Dropdown(\n",
        "        options=img_list,\n",
        "        value='full3',\n",
        "        description='Select Image:',\n",
        "        disabled=False,\n",
        "    )\n",
        "\n",
        "    # Create a dropdown for selecting example audio\n",
        "    example_audio = widgets.Dropdown(\n",
        "        options=audio_list,\n",
        "        value='RD_Radio31_000',\n",
        "        description='Select Audio:',\n",
        "        disabled=False,\n",
        "    )\n",
        "\n",
        "    # Add event handlers\n",
        "    image_source.observe(select_image_audio, names='value')\n",
        "    example_images.observe(select_image_audio, names='value')\n",
        "    audio_source.observe(select_image_audio, names='value')\n",
        "    example_audio.observe(select_image_audio, names='value')\n",
        "\n",
        "    # Display widgets\n",
        "    # display(image_source)\n",
        "    display(example_images)\n",
        "    # display(audio_source)\n",
        "    display(example_audio)\n",
        "    display(output)\n",
        "\n",
        "    # Initial call to select_image_audio to display the default values\n",
        "    select_image_audio(None)\n",
        "\n",
        "    return selected_img, selected_audio\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def create_upload_widgets():\n",
        "    selected_img = None\n",
        "    selected_audio = None\n",
        "    output = widgets.Output()\n",
        "    progress_bar_audio = widgets.IntProgress(description='Uploading Audio:', max=100, style={'bar_color': 'green'})  # Progress bar for audio only\n",
        "\n",
        "    # Function to handle image upload\n",
        "    def on_img_upload(change):\n",
        "        nonlocal selected_img\n",
        "        # Process the uploaded image file\n",
        "        for name, file_info in uploader_img.value.items():\n",
        "            selected_img = f\"/content/{name}\"\n",
        "            with open(selected_img, \"wb\") as output_file:\n",
        "                output_file.write(file_info['content'])\n",
        "            display_img(selected_img)  # Display the uploaded image\n",
        "\n",
        "    # Function to handle audio upload\n",
        "    def on_audio_upload(change):\n",
        "        nonlocal selected_audio\n",
        "        # Display the progress bar for audio upload\n",
        "        progress_bar_audio.value = 0  # Reset progress bar\n",
        "        display(progress_bar_audio)\n",
        "\n",
        "        # Process the uploaded audio file\n",
        "        for name, file_info in uploader_audio.value.items():\n",
        "            selected_audio = f\"/content/{name}\"\n",
        "            with open(selected_audio, \"wb\") as output_file:\n",
        "                file_size = len(file_info['content'])\n",
        "                chunk_size = file_size // 100  # Update the progress for each 1%\n",
        "                for i in range(0, file_size, chunk_size):\n",
        "                    output_file.write(file_info['content'][i:i+chunk_size])\n",
        "                    progress_bar_audio.value = (i / file_size) * 100\n",
        "                progress_bar_audio.value = 100  # Mark as complete\n",
        "            with output:\n",
        "                clear_output(wait=True)\n",
        "                print(f'Selected Audio: {selected_audio}')  # Display the selected audio\n",
        "\n",
        "    # Function to display the selected image\n",
        "    def display_img(file_path):\n",
        "        if file_path and os.path.isfile(file_path):\n",
        "            img = plt.imread(file_path)\n",
        "            plt.imshow(img)\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "            print(f'Selected Image: {file_path}')\n",
        "        else:\n",
        "            print(\"No image selected or file path is invalid.\")\n",
        "\n",
        "    # Create file upload widget for images\n",
        "    uploader_img = widgets.FileUpload(\n",
        "        accept='.png',  # Accept only .png files\n",
        "        multiple=False  # Allow only one file to be uploaded\n",
        "    )\n",
        "    uploader_img.observe(on_img_upload, names='value')\n",
        "\n",
        "    # Create file upload widget for audio\n",
        "    uploader_audio = widgets.FileUpload(\n",
        "        accept='.wav',  # Accept only .wav files\n",
        "        multiple=False  # Allow only one file to be uploaded\n",
        "    )\n",
        "    uploader_audio.observe(on_audio_upload, names='value')\n",
        "\n",
        "    # Display widgets\n",
        "    display(widgets.Label('Upload an image (.png):'))\n",
        "    display(uploader_img)\n",
        "    display(widgets.Label('Upload an audio file (.wav):'))\n",
        "    display(uploader_audio)\n",
        "    display(output)  # The output is displayed only once\n",
        "\n",
        "    return uploader_img, uploader_audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ4CplXsYl_E"
      },
      "outputs": [],
      "source": [
        "### make sure that CUDA is available in Edit -> Nootbook settings -> GPU\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mdq6j4E5KQAR"
      },
      "outputs": [],
      "source": [
        "# This step will take 4-5 minutes\n",
        "start = start_time()\n",
        "setup_environment()\n",
        "print('Download pre-trained models...')\n",
        "!rm -rf checkpoints\n",
        "!bash scripts/download_models.sh\n",
        "clear_output(wait=True)\n",
        "elapsed_time(start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKzxFleg6MgQ"
      },
      "source": [
        "## Select your source images and driven audio\n",
        "\n",
        "\n",
        "\n",
        "1. **Source Image:** The source image is typically an image or a series of images of a person's face. This image serves as the visual reference or template for the deepfake. The deepfake algorithm will manipulate and replace the face in the target video with the face from the source image, making it appear as though the person in the source image is speaking or acting in the target video.\n",
        "\n",
        "2. **Driven Audio:** The driven audio is the audio content that you want to synchronize with the deepfake video. It can be a recording of someone speaking, singing, or any audio content you want to associate with the deepfake. The deepfake algorithm analyzes the audio's speech patterns and uses this information to animate the mouth movements of the face in the target video so that it appears to be speaking or syncing with the audio.\n",
        "\n",
        "Together, these elements allow deepfake technology to create videos where a person in the source image appears to say or do things that align with the content of the driven audio, making it look like the source person is delivering the audio's message or performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP9MhXxQ9m6B"
      },
      "source": [
        "Use this code if you just want to use the example images and audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipPpg7SakjZj"
      },
      "outputs": [],
      "source": [
        "# Define the directory containing audio files\n",
        "audio_directory = 'examples/driven_audio'\n",
        "audio_list = [os.path.splitext(file)[0] for file in os.listdir(audio_directory) if file.endswith('.wav')]\n",
        "audio_list.sort()\n",
        "\n",
        "# Define a list of image options\n",
        "# img_list = ['full1', 'full2', 'full3']\n",
        "img_directory = 'examples/source_image'\n",
        "img_list = [os.path.splitext(file)[0] for file in os.listdir(img_directory) if file.endswith('.png')]\n",
        "img_list.sort()\n",
        "\n",
        "# Call the create_image_audio_selector function with img_list and audio_list\n",
        "selected_img, selected_audio = create_image_audio_selector(img_list, audio_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpetlP6W9wIy"
      },
      "source": [
        "Use this code if you want to upload your own images and audio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function to create upload widgets\n",
        "uploader_img, uploader_audio = create_upload_widgets()"
      ],
      "metadata": {
        "id": "UjeDWVIhKO7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-khNZcnGK4UK"
      },
      "source": [
        "## Animation Time!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this cell if you selected audio from example/driven_audio and if you only want to animate the face\n",
        "start = start_time()\n",
        "print(selected_img)\n",
        "!python3.8 inference.py --driven_audio {selected_audio} --source_image {selected_img} --result_dir ./results --enhancer gfpgan\n",
        "elapsed_time(start)"
      ],
      "metadata": {
        "id": "ePDRylT_MGn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this cell if you selected audio from example/driven_audio and if you want to animate the face with a natural full body video\n",
        "start = start_time()\n",
        "print(selected_img)\n",
        "!python3.8 inference.py --driven_audio {selected_audio} --source_image {selected_img} --result_dir ./results --still --preprocess full --enhancer gfpgan\n",
        "elapsed_time(start)"
      ],
      "metadata": {
        "id": "w6nJbbwXMGD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this cell if you uploaded audio and if you only want to animate the face\n",
        "start = start_time()\n",
        "print(uploader_img)\n",
        "!python3.8 inference.py --driven_audio {uploader_audio} --source_image {uploader_img} --result_dir ./results --enhancer gfpgan\n",
        "elapsed_time(start)"
      ],
      "metadata": {
        "id": "0MoXzqMaMFYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToBlDusjK5sS"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell if you uploaded audio and if you want to animate the face with a natural full body video\n",
        "start = start_time()\n",
        "print(uploader_img)\n",
        "!python3.8 inference.py --driven_audio {uploader_audio} --source_image {uploader_img} --result_dir ./results --still --preprocess full --enhancer gfpgan\n",
        "elapsed_time(start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNTgOWUgmi-0"
      },
      "outputs": [],
      "source": [
        "#@title Here are the filenames for all of your generated results\n",
        "results = sorted(os.listdir('./results/'))\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose a file to view and download from the results listed above. By default, the first file is selected. To view the second file, enter '1' in the space provided. To select a file, always subtract one from its position in the list. For example, to select the third file, you would enter '2'.\n",
        "video_path = glob.glob('./results/*.mp4')[0]\n",
        "\n",
        "# Display the video with a download link\n",
        "display_video_with_download_link(video_path)\n"
      ],
      "metadata": {
        "id": "ta3TIVptI-5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Credits\n",
        "\n",
        "**SadTalker dev team** - Original SadTalker software developers and original colab authors: Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, Fei Wang. Xi'an Jiaotong University, Tencent AI Lab, Ant Group. Click here for more info: <br>\n",
        "**Artificial Nouveau** updated the notebook for workshops\n",
        "\n",
        "\n",
        "---------------\n",
        "\n",
        "Backup model archive (outdated): https://huggingface.co/QuickWick/Music-AI-Voices/tree/main"
      ],
      "metadata": {
        "id": "hZoJPzUTI20j"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "db5031b3636a3f037ea48eb287fd3d023feb9033aefc2a9652a92e470fb0851b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}